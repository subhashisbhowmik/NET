'''
	Module containing Error Functions.
	Classes embody Cost Functions for Minimization,
	usually an elementwise mapping.
'''
import numpy

class Error:
	'''
		Base Class for Error Functions
	'''
	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		self.inputs = inputs
		self.outputs = self.inputs
		self.previousinput = None
		self.previousoutput = None
		self.function = None
		self.derivative = None

	def feedforward(self, inputvector):
		'''
			Method to feedforward a vector through the layer
			: param inputvector : output vector generated by model
			: returns : output vector generated by model
		'''
		self.previousinput = inputvector
		self.previousoutput = self.function(self.previousinput)
		return self.previousoutput

	def backpropagate(self, outputvector):
		'''
			Method to backpropagate derivatives through the layer
			: param outputvector : target vector in training set
			: returns : backpropagated vector mapped to model's output feature space
		'''
		return self.derivative(self.previousoutput, outputvector)

class MeanSquared(Error):
	'''
		Half Mean Squared Error Function
		Mathematically, f(y, o)(i) = 1 / 2 * (y(i) - o(i)) ^ 2
	'''
	criterion = numpy.vectorize(lambda x, y: 0.5 * (x - y) ** 2)

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)
		self.function = lambda x: x
		self.derivative = numpy.subtract

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return MeanSquared.criterion(inputvector, outputvector)

class CrossEntropy(Error):
	'''
		Cross Entropy Error Function
		Mathematically, f(y, o)(i) = - (o(i) * log(y(i)) + (1 - o(i)) * log(1 - y(i)))
	'''
	epsilon = 0.0001
	criterion = numpy.vectorize(lambda x, y: - (y * numpy.log(x + CrossEntropy.epsilon) + (1.0 - y) * numpy.log(1.0 - x + CrossEntropy.epsilon)))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)
		self.function = lambda x: x
		self.derivative = numpy.vectorize(lambda x, y: (1.0 - y) / (1.0 - x + CrossEntropy.epsilon) - y / (x + CrossEntropy.epsilon))

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return CrossEntropy.criterion(inputvector, outputvector)

class NegativeLogLikelihood(Error):
	'''
		Negative Log Likelihood Error Function
		Mathematically, f(y, o)(i) = - o(i) * log(y(i))
	'''
	epsilon = 0.0001
	criterion = numpy.vectorize(lambda x, y: - y * numpy.log(x + NegativeLogLikelihood.epsilon))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)
		self.function = lambda x: x
		self.derivative = numpy.vectorize(lambda x, y: - y / (x + NegativeLogLikelihood.epsilon))

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return NegativeLogLikelihood.criterion(inputvector, outputvector)

class CrossSigmoid(Error):
	'''
		Cross Entropy of Sigmoid Transfer Error Function
		Mathematically, f(y, o)(i) = - (o(i) * log(g(y)(i)) + (1 - o(i)) * log(1 - g(y)(i)))
						g(y)(i) = 1 / (1 + exp(-y(i)))
	'''
	epsilon = 0.0001
	criterion = numpy.vectorize(lambda x, y: - (y * numpy.log(x + CrossSigmoid.epsilon) + (1.0 - y) * numpy.log(1.0 - x + CrossSigmoid.epsilon)))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)
		self.function = numpy.vectorize(lambda x: 1.0 / (1.0 + numpy.exp(-x)))
		self.derivative = numpy.subtract

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return CrossSigmoid.criterion(inputvector, outputvector)

class LogSoftMax(Error):
	'''
		Negative Log Likelihood of Soft Max Transfer Error Function
		Mathematically, f(y, o)(i) = - o(i) * log(g(y)(i))
						g(y)(i) = exp(y(i)) / sum_over_j(exp(y(j)))
	'''
	epsilon = 0.0001
	criterion = numpy.vectorize(lambda x, y: - y * numpy.log(x + LogSoftMax.epsilon))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)
		self.derivative = numpy.subtract

	def feedforward(self, inputvector):
		'''
			Method to feedforward a vector through the layer
			: param inputvector : output vector generated by model
			: returns : output vector generated by model
		'''
		self.previousinput = inputvector
		inputvector = numpy.subtract(inputvector, numpy.amax(inputvector))
		inputvector = numpy.exp(inputvector)
		self.previousoutput = numpy.divide(inputvector, numpy.sum(inputvector))
		return self.previousoutput

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return LogSoftMax.criterion(inputvector, outputvector)

class KullbackLeiblerDivergence(Error):
	'''
		Kullback Leibler Divergence Error Function
		Mathematically, f(y, o)(i) = o(i) * log(o(i) / y(i))
	'''
	epsilon = 0.0001
	criterion = numpy.vectorize(lambda x, y: y * (numpy.log(y + KullbackLeiblerDivergence.epsilon) - numpy.log(x + KullbackLeiblerDivergence.epsilon)))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)
		self.function = lambda x: x
		self.derivative = numpy.vectorize(lambda x, y: - y / (x + KullbackLeiblerDivergence.epsilon))

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return KullbackLeiblerDivergence.criterion(inputvector, outputvector)

class CosineDistance(Error):
	'''
		Cosine Distance Error Function
		Mathematically, f(y, o)(i) = - o(i) * y(i) / (sum_over_j(o(j) ^ 2) * sum_over_j(y(j) ^ 2)) ^ 0.5
	'''
	epsilon = 0.0001

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)
		self.function = lambda x: x

	def backpropagate(self, outputvector):
		'''
			Method to backpropagate derivatives through the layer
			: param outputvector : target vector in training set
			: returns : backpropagated vector mapped to model's output feature space
		'''
		inputnorm = numpy.sum(numpy.square(self.previousinput))
		outputnorm = numpy.sum(numpy.square(outputvector))
		direction = numpy.sum(numpy.multiply(self.previousinput, outputvector))
		return numpy.divide(numpy.subtract(numpy.multiply(direction, self.previousinput), numpy.multiply(inputnorm, outputvector)), numpy.sqrt(outputnorm) * numpy.sqrt(inputnorm) ** 3)

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		inputnorm = numpy.sqrt(numpy.sum(numpy.square(inputvector)))
		outputnorm = numpy.sqrt(numpy.sum(numpy.square(outputvector)))
		direction = numpy.multiply(inputvector, outputvector)
		return - numpy.divide(direction, (inputnorm * outputnorm + CosineDistance.epsilon))

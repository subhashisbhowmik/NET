'''
	Module containing Error Functions.
	Classes embody Cost Functions for Minimization,
	usually an elementwise mapping.
'''
import math
from . import base, configure

class Error(base.Net):
	'''
		Base Class for Error Functions
	'''
	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		if not hasattr(self, 'dimensions'):
			self.dimensions = dict()
		self.dimensions['inputs'] = inputs
		self.dimensions['outputs'] = self.dimensions['inputs']
		if not hasattr(self, 'history'):
			self.history = dict()
		self.history['input'] = list()
		self.history['output'] = list()
		self.__finit__()

	def __finit__(self):
		'''
			Internal Method used to initialize function attributes
		'''
		if not hasattr(self, 'functions'):
			self.functions = dict()
		self.functions['function'] = None
		self.functions['derivative'] = None

	def feedforward(self, inputvector):
		'''
			Method to feedforward a vector through the layer
			: param inputvector : output vector generated by model
			: returns : output vector generated by model
		'''
		if inputvector.shape != (self.dimensions['inputs'], 1):
			self.dimensionsError(self.__class__.__name__)
		self.history['input'].append(inputvector)
		self.history['output'].append(self.functions['function'](self.history['input'][-1]))
		return self.history['output'][-1]

	def backpropagate(self, outputvector):
		'''
			Method to backpropagate derivatives through the layer
			: param outputvector : target vector in training set
			: returns : backpropagated vector mapped to model's output feature space
		'''
		if outputvector.shape != (self.dimensions['outputs'], 1):
			self.dimensionsError(self.__class__.__name__)
		self.history['input'].pop()
		return self.functions['derivative'](self.history['output'].pop(), outputvector)

class MeanSquared(Error):
	'''
		Half Mean Squared Error Function
		Mathematically, f(y, o)(i) = 1 / 2 * (y(i) - o(i)) ^ 2
	'''
	criterion = configure.functions['vectorize'](lambda x, y: 0.5 * (x - y) ** 2)

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)

	def __finit__(self):
		'''
			Internal Method used to initialize function attributes
		'''
		if not hasattr(self, 'functions'):
			self.functions = dict()
		self.functions['function'] = lambda x: x
		self.functions['derivative'] = configure.functions['subtract']

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return MeanSquared.criterion(inputvector, outputvector)

class CrossEntropy(Error):
	'''
		Cross Entropy Error Function
		Mathematically, f(y, o)(i) = - (o(i) * log(y(i)) + (1 - o(i)) * log(1 - y(i)))
	'''
	epsilon = 0.0001
	criterion = configure.functions['vectorize'](lambda x, y: - (y * math.log(x + CrossEntropy.epsilon) + (1.0 - y) * math.log(1.0 - x + CrossEntropy.epsilon)))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)

	def __finit__(self):
		'''
			Internal Method used to initialize function attributes
		'''
		if not hasattr(self, 'functions'):
			self.functions = dict()
		self.functions['function'] = lambda x: x
		self.functions['derivative'] = configure.functions['vectorize'](lambda x, y: (1.0 - y) / (1.0 - x + CrossEntropy.epsilon) - y / (x + CrossEntropy.epsilon))

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return CrossEntropy.criterion(inputvector, outputvector)

class NegativeLogLikelihood(Error):
	'''
		Negative Log Likelihood Error Function
		Mathematically, f(y, o)(i) = - o(i) * log(y(i))
	'''
	epsilon = 0.0001
	criterion = configure.functions['vectorize'](lambda x, y: - y * math.log(x + NegativeLogLikelihood.epsilon))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)

	def __finit__(self):
		'''
			Internal Method used to initialize function attributes
		'''
		if not hasattr(self, 'functions'):
			self.functions = dict()
		self.functions['function'] = lambda x: x
		self.functions['derivative'] = configure.functions['vectorize'](lambda x, y: - y / (x + NegativeLogLikelihood.epsilon))

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return NegativeLogLikelihood.criterion(inputvector, outputvector)

class CrossSigmoid(Error):
	'''
		Cross Entropy of Sigmoid Transfer Error Function
		Mathematically, f(y, o)(i) = - (o(i) * log(g(y)(i)) + (1 - o(i)) * log(1 - g(y)(i)))
						g(y)(i) = 1 / (1 + exp(-y(i)))
	'''
	epsilon = 0.0001
	criterion = configure.functions['vectorize'](lambda x, y: - (y * math.log(x + CrossSigmoid.epsilon) + (1.0 - y) * math.log(1.0 - x + CrossSigmoid.epsilon)))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)

	def __finit__(self):
		'''
			Internal Method used to initialize function attributes
		'''
		if not hasattr(self, 'functions'):
			self.functions = dict()
		self.functions['function'] = configure.functions['vectorize'](lambda x: 1.0 / (1.0 + math.exp(-x)))
		self.functions['derivative'] = configure.functions['subtract']

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return CrossSigmoid.criterion(inputvector, outputvector)

class LogSoftMax(Error):
	'''
		Negative Log Likelihood of Soft Max Transfer Error Function
		Mathematically, f(y, o)(i) = - o(i) * log(g(y)(i))
						g(y)(i) = exp(y(i)) / sum_over_j(exp(y(j)))
	'''
	epsilon = 0.0001
	criterion = configure.functions['vectorize'](lambda x, y: - y * math.log(x + LogSoftMax.epsilon))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)

	def __finit__(self):
		'''
			Internal Method used to initialize function attributes
		'''
		if not hasattr(self, 'functions'):
			self.functions = dict()
		self.functions['derivative'] = configure.functions['subtract']

	def feedforward(self, inputvector):
		'''
			Method to feedforward a vector through the layer
			: param inputvector : output vector generated by model
			: returns : output vector generated by model
		'''
		if inputvector.shape != (self.dimensions['inputs'], 1):
			self.dimensionsError(self.__class__.__name__)
		self.history['input'].append(inputvector)
		inputvector = configure.functions['subtract'](inputvector, configure.functions['amax'](inputvector))
		inputvector = configure.functions['exp'](inputvector)
		self.history['output'].append(configure.functions['divide'](inputvector, configure.functions['sum'](inputvector)))
		return self.history['output'][-1]

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return LogSoftMax.criterion(inputvector, outputvector)

class KullbackLeiblerDivergence(Error):
	'''
		Kullback Leibler Divergence Error Function
		Mathematically, f(y, o)(i) = o(i) * log(o(i) / y(i))
	'''
	epsilon = 0.0001
	criterion = configure.functions['vectorize'](lambda x, y: y * (math.log(y + KullbackLeiblerDivergence.epsilon) - math.log(x + KullbackLeiblerDivergence.epsilon)))

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)

	def __finit__(self):
		'''
			Internal Method used to initialize function attributes
		'''
		if not hasattr(self, 'functions'):
			self.functions = dict()
		self.functions['function'] = lambda x: x
		self.functions['derivative'] = configure.functions['vectorize'](lambda x, y: - y / (x + KullbackLeiblerDivergence.epsilon))

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		return KullbackLeiblerDivergence.criterion(inputvector, outputvector)

class CosineDistance(Error):
	'''
		Cosine Distance Error Function
		Mathematically, f(y, o)(i) = - o(i) * y(i) / (sum_over_j(o(j) ^ 2) * sum_over_j(y(j) ^ 2)) ^ 0.5
	'''
	epsilon = 0.0001

	def __init__(self, inputs):
		'''
			Constructor
			: param inputs : dimension of input (and output) feature space
		'''
		Error.__init__(self, inputs)

	def __finit__(self):
		'''
			Internal Method used to initialize function attributes
		'''
		if not hasattr(self, 'functions'):
			self.functions = dict()
		self.functions['function'] = lambda x: x

	def backpropagate(self, outputvector):
		'''
			Method to backpropagate derivatives through the layer
			: param outputvector : target vector in training set
			: returns : backpropagated vector mapped to model's output feature space
		'''
		if outputvector.shape != (self.dimensions['outputs'], 1):
			self.dimensionsError(self.__class__.__name__)
		self.history['output'].pop()
		inputnorm = configure.functions['sum'](configure.functions['square'](self.history['input'][-1]))
		outputnorm = configure.functions['sum'](configure.functions['square'](outputvector))
		direction = configure.functions['sum'](configure.functions['multiply'](self.history['input'][-1], outputvector))
		return configure.functions['divide'](configure.functions['subtract'](configure.functions['multiply'](direction, self.history['input'].pop()), configure.functions['multiply'](inputnorm, outputvector)), configure.functions['sqrt'](outputnorm) * configure.functions['sqrt'](inputnorm) ** 3)

	@staticmethod
	def compute(inputvector, outputvector):
		'''
			Static method to compute the cost elementwise error
			: param inputvector : output vector generated by model
			: param outputvector : target vector in training set
			: returns : elementwise error between output and target vectors
		'''
		inputnorm = configure.functions['sqrt'](configure.functions['sum'](configure.functions['square'](inputvector)))
		outputnorm = configure.functions['sqrt'](configure.functions['sum'](configure.functions['square'](outputvector)))
		direction = configure.functions['multiply'](inputvector, outputvector)
		return - configure.functions['divide'](direction, (inputnorm * outputnorm + CosineDistance.epsilon))
